{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2623f33",
   "metadata": {},
   "source": [
    "# Installations\n",
    "!pip install scikit-learn pandas tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929a8ae",
   "metadata": {},
   "source": [
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8899952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89eba8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065760bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HC\\OneDrive\\Desktop\\Semester 6\\Data Science\\dataset.csv\")\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b952bd4",
   "metadata": {},
   "source": [
    "# Features and Labels (adjust column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20eb562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"type_blocker\", \"type_regression\", \"type_bug\"])  # Example labels\n",
    "y = df[[\"type_blocker\", \"type_regression\", \"type_bug\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37532f48",
   "metadata": {},
   "source": [
    "# Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6463d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X = X.drop(columns=[\"report\"])\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, \"scaler.pkl\")  # Save for Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610d08d",
   "metadata": {},
   "source": [
    "\n",
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16761492",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d3999",
   "metadata": {},
   "source": [
    "\n",
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eff37ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Report:\n",
      "Hamming Loss: 0.05755395683453238\n",
      "Micro-F1: 0.8628571428571429\n",
      "Macro-F1: 0.6705606342528921\n",
      "\n",
      "SVM Report:\n",
      "Hamming Loss: 0.05515587529976019\n",
      "Micro-F1: 0.8707865168539326\n",
      "Macro-F1: 0.7086784689513291\n",
      "\n",
      "Online Perceptron Report:\n",
      "Hamming Loss: 0.06235011990407674\n",
      "Micro-F1: 0.8433734939759037\n",
      "Macro-F1: 0.4795458993932276\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": MultiOutputClassifier(LogisticRegression(max_iter=1000)),\n",
    "    \"SVM\": MultiOutputClassifier(SVC(probability=True)),\n",
    "    \"Online Perceptron\": MultiOutputClassifier(SGDClassifier(loss='perceptron', eta0=1, learning_rate='constant'))\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n{name} Report:\")\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_test, preds))\n",
    "    print(\"Micro-F1:\", f1_score(y_test, preds, average=\"micro\"))\n",
    "    print(\"Macro-F1:\", f1_score(y_test, preds, average=\"macro\"))\n",
    "    joblib.dump(model, f\"{name.lower().replace(' ', '_')}_defect_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0804e53d",
   "metadata": {},
   "source": [
    "# DNN for Multi-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68fd23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.4153 - loss: 0.4154 - val_accuracy: 0.5135 - val_loss: 0.4959\n",
      "Epoch 2/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6253 - loss: 0.1493 - val_accuracy: 0.4505 - val_loss: 0.4036\n",
      "Epoch 3/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5714 - loss: 0.1225 - val_accuracy: 0.4505 - val_loss: 0.3540\n",
      "Epoch 4/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5297 - loss: 0.1320 - val_accuracy: 0.4595 - val_loss: 0.3125\n",
      "Epoch 5/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5195 - loss: 0.1192 - val_accuracy: 0.4505 - val_loss: 0.2735\n",
      "Epoch 6/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5239 - loss: 0.1078 - val_accuracy: 0.4595 - val_loss: 0.2421\n",
      "Epoch 7/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5153 - loss: 0.1161 - val_accuracy: 0.4595 - val_loss: 0.2136\n",
      "Epoch 8/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4736 - loss: 0.1086 - val_accuracy: 0.4595 - val_loss: 0.1895\n",
      "Epoch 9/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5023 - loss: 0.1134 - val_accuracy: 0.4595 - val_loss: 0.1698\n",
      "Epoch 10/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5498 - loss: 0.1032 - val_accuracy: 0.4369 - val_loss: 0.1520\n",
      "Epoch 11/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5105 - loss: 0.1177 - val_accuracy: 0.4595 - val_loss: 0.1428\n",
      "Epoch 12/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5448 - loss: 0.0990 - val_accuracy: 0.4595 - val_loss: 0.1324\n",
      "Epoch 13/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4956 - loss: 0.0979 - val_accuracy: 0.4595 - val_loss: 0.1296\n",
      "Epoch 14/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5128 - loss: 0.1256 - val_accuracy: 0.4595 - val_loss: 0.1250\n",
      "Epoch 15/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5128 - loss: 0.1011 - val_accuracy: 0.4595 - val_loss: 0.1233\n",
      "Epoch 16/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5128 - loss: 0.1118 - val_accuracy: 0.4459 - val_loss: 0.1233\n",
      "Epoch 17/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4898 - loss: 0.0906 - val_accuracy: 0.4369 - val_loss: 0.1249\n",
      "Epoch 18/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4449 - loss: 0.1052 - val_accuracy: 0.4459 - val_loss: 0.1220\n",
      "Epoch 19/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4491 - loss: 0.1119 - val_accuracy: 0.4459 - val_loss: 0.1241\n",
      "Epoch 20/20\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5005 - loss: 0.1090 - val_accuracy: 0.4459 - val_loss: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "DNN Report:\n",
      "Hamming Loss: 0.05515587529976019\n",
      "Micro-F1: 0.8707865168539326\n",
      "Macro-F1: 0.7086784689513291\n"
     ]
    }
   ],
   "source": [
    "model_dnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(y.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_dnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_dnn.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "model_dnn.save(\"defect_dnn.h5\")\n",
    "\n",
    "# Evaluate DNN\n",
    "dnn_preds = (model_dnn.predict(X_test) > 0.5).astype(int)\n",
    "print(\"\\nDNN Report:\")\n",
    "print(\"Hamming Loss:\", hamming_loss(y_test, dnn_preds))\n",
    "print(\"Micro-F1:\", f1_score(y_test, dnn_preds, average=\"micro\"))\n",
    "print(\"Macro-F1:\", f1_score(y_test, dnn_preds, average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
