{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c2a930",
   "metadata": {},
   "source": [
    "# Step 1: Install Required Libraries\n",
    "!pip install datasets librosa scikit-learn matplotlib seaborn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bdaf7",
   "metadata": {},
   "source": [
    "# Step 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28cd6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c727e9b",
   "metadata": {},
   "source": [
    "# Step 3: Load Urdu Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f85d40a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 6794/6794 [00:44<00:00, 153.93files/s] \n",
      "Generating train split: 100%|██████████| 6794/6794 [00:16<00:00, 419.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"CSALT/deepfake_detection_dataset_urdu\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700a707",
   "metadata": {},
   "source": [
    "# Step 4: Feature Extraction (MFCCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08d41a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(batch, max_len=100):\n",
    "    features, labels = [], []\n",
    "    for example in batch:\n",
    "        audio_path = example['audio']['path']  # Correct path reference\n",
    "        # Load audio using librosa\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        if mfcc.shape[1] < max_len:\n",
    "            mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])))\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_len]\n",
    "        \n",
    "        features.append(mfcc.flatten())  # Flatten for classical models\n",
    "\n",
    "        # Infer the label from the folder name (e.g., 'Bonafide' -> 0, 'Spoof' -> 1)\n",
    "        label = 1 if 'Spoof' in audio_path else 0  # Adjust based on folder structure\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "X, y = extract_features(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795cc65",
   "metadata": {},
   "source": [
    "# Step 5: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feb9b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f45c0a",
   "metadata": {},
   "source": [
    "# Step 6: Classical ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f6c1c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       681\n",
      "           1       0.94      0.95      0.94       678\n",
      "\n",
      "    accuracy                           0.94      1359\n",
      "   macro avg       0.94      0.94      0.94      1359\n",
      "weighted avg       0.94      0.94      0.94      1359\n",
      "\n",
      "ROC AUC: 0.9891427234805661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80       681\n",
      "           1       0.81      0.78      0.79       678\n",
      "\n",
      "    accuracy                           0.80      1359\n",
      "   macro avg       0.80      0.80      0.80      1359\n",
      "weighted avg       0.80      0.80      0.80      1359\n",
      "\n",
      "ROC AUC: 0.8825516873936038\n",
      "\n",
      "Perceptron Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.82       681\n",
      "           1       0.90      0.66      0.76       678\n",
      "\n",
      "    accuracy                           0.79      1359\n",
      "   macro avg       0.82      0.79      0.79      1359\n",
      "weighted avg       0.81      0.79      0.79      1359\n",
      "\n",
      "ROC AUC: 0.9156324856297567\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"SVM\": SVC(probability=True, kernel='rbf'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Perceptron\": Perceptron()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    probas = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_test)\n",
    "    print(f\"\\n{name} Report:\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, probas))\n",
    "    joblib.dump(model, f\"{name.lower().replace(' ', '_')}_audio_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd70a0f",
   "metadata": {},
   "source": [
    "# Step 7: Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "228bf611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.8008 - loss: 0.4181 - val_accuracy: 0.7579 - val_loss: 0.7281\n",
      "Epoch 2/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9231 - loss: 0.1905 - val_accuracy: 0.8698 - val_loss: 0.3246\n",
      "Epoch 3/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9492 - loss: 0.1388 - val_accuracy: 0.9735 - val_loss: 0.0863\n",
      "Epoch 4/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9627 - loss: 0.0986 - val_accuracy: 0.9676 - val_loss: 0.0895\n",
      "Epoch 5/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9562 - loss: 0.1165 - val_accuracy: 0.9632 - val_loss: 0.0969\n",
      "Epoch 6/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9562 - loss: 0.1158 - val_accuracy: 0.9441 - val_loss: 0.1553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model\n",
    "model_dnn = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_dnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_dnn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfaae9c",
   "metadata": {},
   "source": [
    "# Evaluate DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d62832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      "DNN Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       681\n",
      "           1       0.98      0.96      0.97       678\n",
      "\n",
      "    accuracy                           0.97      1359\n",
      "   macro avg       0.97      0.97      0.97      1359\n",
      "weighted avg       0.97      0.97      0.97      1359\n",
      "\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9962466267288691\n"
     ]
    }
   ],
   "source": [
    "dnn_preds = (model_dnn.predict(X_test) > 0.5).astype(int)\n",
    "print(\"\\nDNN Report:\")\n",
    "print(classification_report(y_test, dnn_preds))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, model_dnn.predict(X_test)))\n",
    "model_dnn.save(\"deepfake_dnn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
